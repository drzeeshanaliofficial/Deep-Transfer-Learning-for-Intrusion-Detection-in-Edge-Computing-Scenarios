from google.colab import drive
import pandas as pd
import numpy as np

drive.mount('/content/drive')

path1 = "/content/drive/My Drive/CIC IDS main/"

import numpy as np 
import pandas as pd 
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
nRowsRead = None

df1=pd.read_csv(path1+"Monday-WorkingHours.pcap_ISCX.csv")

df2=pd.read_csv(path1+"Tuesday-WorkingHours.pcap_ISCX.csv")

df3=pd.read_csv(path1+"Wednesday-workingHours.pcap_ISCX.csv")

df4=pd.read_csv(path1+"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv")
df5=pd.read_csv(path1+"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv")

df6=pd.read_csv(path1+"Friday-WorkingHours-Morning.pcap_ISCX.csv")
df7=pd.read_csv(path1+"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv")
df8=pd.read_csv(path1+"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv")


df = pd.concat([df1,df2])
del df1,df2
df = pd.concat([df,df3])
del df3
df = pd.concat([df,df4])
del df4
df = pd.concat([df,df5])
del df5
df = pd.concat([df,df6])
del df6
df = pd.concat([df,df7])
del df7
df = pd.concat([df,df8])
del df8

nRow, nCol = df.shape
print(df.shape)

import pandas as pd


df['Flow Bytes/s'] = df['Flow Bytes/s'].fillna(0)

print(df.isnull().sum())

import pandas as pd


print("Shape before duplicate samples removal:", df.shape)

duplicate_samples = df[df.duplicated()]

print("Duplicate Samples Count:", len(duplicate_samples))

df = df.drop_duplicates()

duplicate_samples_after_removal = df[df.duplicated()]

print("Shape after duplicate samples removal:", df.shape)

label_mapping = {
    'FTP-Patator':'Brute Force',
    'SSH-Patator':'Brute Force',

    'DoS GoldenEye': 'DoS',
    'DoS slowloris': 'DoS',
    'DoS Slowhttptest': 'DoS',
    'DoS Hulk': 'DoS',
    'Heartbleed':'DoS',
    'Bot':'DoS',

    'Web Attack � Brute Force': 'Web Attacks',
    'Web Attack � XSS': 'Web Attacks',
    'Web Attack � Sql Injection': 'Web Attacks',
}

df[' Label']=df[' Label'].replace(label_mapping)

sampling_strategy_over = {
    "Brute Force": 12000,        
    "DoS": 205000,                
    "Web Attacks": 10000,           
    "Infiltration": 1500,                
    "PortScan": 100000,                  
    "DDoS": 150000,                     
}

sampling_strategy_under = {
    "BENIGN": 500000                     
}

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy_under)
over_sampler = RandomOverSampler(sampling_strategy=sampling_strategy_over)

X = df.drop(' Label', axis=1).values
y = df[' Label'].values

X_under, y_under = under_sampler.fit_resample(X, y)

X_resampled, y_resampled = over_sampler.fit_resample(X_under, y_under)

print(X_resampled.shape)
print(y_resampled.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

print("Train set shape:", X_train.shape, y_train.shape)
print("Test set shape:", X_test.shape, y_test.shape)


from transformers import BertTokenizer, TFBertModel
import tensorflow as tf
import numpy as np
from tqdm import tqdm

flow_text_train = [' '.join(map(str, row)) for row in X_train]

print("Number of Rows in flow_text_train:", len(flow_text_train))

X_train_tokenized = []
print("\nTokenizing the Training Set:")
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
for text in tqdm(flow_text_train):
    tokenized_text = tokenizer.encode_plus(
        text,
        truncation=True,
        padding='max_length',
        max_length=128,
        return_attention_mask=True,
        return_tensors='tf'
    )
    X_train_tokenized.append(tokenized_text)

X_train_input_ids = tf.squeeze(tf.stack([t['input_ids'] for t in X_train_tokenized]), axis=1)
X_train_attention_mask = tf.squeeze(tf.stack([t['attention_mask'] for t in X_train_tokenized]), axis=1)

print("Extracting Features from the Training Set:")
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
X_train_features = []
for i in tqdm(range(0, len(X_train_input_ids), 100)):
    batch_input_ids = X_train_input_ids[i:i+100]
    batch_attention_mask = X_train_attention_mask[i:i+100]
    features = bert_model([batch_input_ids, batch_attention_mask])[0][:, 0, :].numpy()
    X_train_features.append(features)
X_train_features = np.concatenate(X_train_features, axis=0)
print("Train features shape:", X_train_features.shape)

np.save('train_features_BERT_Base.npy', X_train_features)


from transformers import BertTokenizer, TFBertModel
import tensorflow as tf
import numpy as np
from tqdm import tqdm

flow_text_test = [' '.join(map(str, row)) for row in X_test]


print("Number of Rows in flow_text_test:", len(flow_text_test))

X_test_tokenized = []
print("\nTokenizing the Testing Set:")
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
for text in tqdm(flow_text_test):
    tokenized_text = tokenizer.encode_plus(
        text,
        truncation=True,
        padding='max_length',
        max_length=128,
        return_attention_mask=True,
        return_tensors='tf'
    )
    X_test_tokenized.append(tokenized_text)

X_test_input_ids = tf.squeeze(tf.stack([t['input_ids'] for t in X_test_tokenized]), axis=1)
X_test_attention_mask = tf.squeeze(tf.stack([t['attention_mask'] for t in X_test_tokenized]), axis=1)

print("Extracting Features from the Testing Set:")
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
X_test_features = []
for i in tqdm(range(0, len(X_test_input_ids), 100)):
    batch_input_ids = X_test_input_ids[i:i+100]
    batch_attention_mask = X_test_attention_mask[i:i+100]
    features = bert_model([batch_input_ids, batch_attention_mask])[0][:, 0, :].numpy()
    X_test_features.append(features)
X_test_features = np.concatenate(X_test_features, axis=0)
print("Test features shape:", X_test_features.shape)


np.save('test_features_BERT_Base.npy', X_test_features)

import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score, classification_report
import joblib

scaler = joblib.load(path +'cicids_scaler.pkl')
model_2 = tf.keras.models.load_model(path+'meta_model.h5')  


cicids_features = np.load(path1+'test_features_BERT_Base.npy')  
cicids_labels = y_test  

lookup_config_updated = joblib.load(path+'lookup_config_updated.pkl')
lookup_weights_updated = joblib.load(path+'lookup_weights_updated.pkl')
lookup_layer_updated = tf.keras.layers.StringLookup.from_config(lookup_config_updated)
lookup_layer_updated.set_weights(lookup_weights_updated)

cicids_features = scaler.transform(cicids_features)


cicids_labels_encoded = lookup_layer_updated(cicids_labels).numpy()


cicids_predictions = np.argmax(model_2.predict(cicids_features), axis=1)

decoded_cicids_predictions = [lookup_layer_updated.get_vocabulary()[pred] for pred in cicids_predictions]


accuracy_cicids = accuracy_score(cicids_labels, decoded_cicids_predictions)
print("CICIDS Dataset Accuracy:", accuracy_cicids)

unique_labels = list(set(cicids_labels))

report_cicids = classification_report(cicids_labels, decoded_cicids_predictions, labels=unique_labels)
print(report_cicids)

import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
import joblib

accuracy_cicids = accuracy_score(cicids_labels, decoded_cicids_predictions)
print("CICIDS Dataset Accuracy:", accuracy_cicids)

unique_labels = list(set(cicids_labels))

precision, recall, f1_score, _ = precision_recall_fscore_support(cicids_labels, decoded_cicids_predictions, labels=unique_labels, average='weighted')
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1_score)

for label in unique_labels:
    mask = cicids_labels == label
    accuracy = accuracy_score(cicids_labels[mask], np.array(decoded_cicids_predictions)[mask])
    print(f"Accuracy for Attack Type '{label}': {accuracy}")

import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np


unique_labels_no_oov = [label for label in set(cicids_labels) if label in lookup_layer_updated.get_vocabulary()]


mask_no_oov = np.isin(cicids_labels, unique_labels_no_oov) & np.isin(decoded_cicids_predictions, unique_labels_no_oov)

filtered_labels = cicids_labels[mask_no_oov]
filtered_predictions = np.array(decoded_cicids_predictions)[mask_no_oov]

cm_filtered = confusion_matrix(filtered_labels, filtered_predictions)


cm_filtered_percent = cm_filtered.astype('float') / cm_filtered.sum(axis=1)[:, np.newaxis]

class_labels_filtered = np.unique(filtered_labels)

plt.figure(figsize=(8, 5)) 
sns.heatmap(cm_filtered_percent, annot=True, cmap='Blues', fmt='.2%')  

tick_labels_filtered = [f"{class_labels_filtered[i]} ({i})" for i in range(len(class_labels_filtered))]
plt.xticks(np.arange(len(class_labels_filtered)) + 0.5, tick_labels_filtered, rotation=45, ha='right')
plt.yticks(np.arange(len(class_labels_filtered)) + 0.5, tick_labels_filtered, rotation=0)

plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.savefig(path + "confusion_matrix.pdf", format="pdf", bbox_inches='tight')
plt.show()
